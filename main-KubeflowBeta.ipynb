{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48c8d7e-385c-4e46-bafc-1cad3f99f76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyYAML==5.4.1\n",
      "  Using cached PyYAML-5.4.1-cp311-cp311-linux_x86_64.whl\n",
      "Installing collected packages: PyYAML\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "Successfully installed PyYAML-5.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyYAML==5.4.1 --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1585915f-6ab5-4ed8-af1c-c8809fc3f1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==1.8.11\n",
      "  Using cached kfp-1.8.11-py3-none-any.whl\n",
      "Collecting absl-py<2,>=0.9 (from kfp==1.8.11)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.11) (5.4.1)\n",
      "Collecting google-cloud-storage<2,>=1.20.0 (from kfp==1.8.11)\n",
      "  Using cached google_cloud_storage-1.44.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in ./.local/lib/python3.11/site-packages (from kfp==1.8.11) (9.0.0)\n",
      "Collecting google-api-python-client<2,>=1.7.8 (from kfp==1.8.11)\n",
      "  Using cached google_api_python_client-1.12.11-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth<2,>=1.6.1 (from kfp==1.8.11)\n",
      "  Using cached google_auth-1.35.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.11) (0.10.1)\n",
      "Collecting cloudpickle<3,>=2.0.0 (from kfp==1.8.11)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2 (from kfp==1.8.11)\n",
      "  Using cached kfp_server_api-1.8.5-py3-none-any.whl\n",
      "Collecting jsonschema<4,>=3.0.1 (from kfp==1.8.11)\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.11) (0.9.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.11) (8.1.7)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in ./.local/lib/python3.11/site-packages (from kfp==1.8.11) (1.2.14)\n",
      "Collecting strip-hints<1,>=0.1.8 (from kfp==1.8.11)\n",
      "  Using cached strip_hints-0.1.10-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.11) (0.15)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.13 (from kfp==1.8.11)\n",
      "  Using cached kfp_pipeline_spec-0.1.16-py3-none-any.whl.metadata (323 bytes)\n",
      "Collecting fire<1,>=0.3.1 (from kfp==1.8.11)\n",
      "  Using cached fire-0.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.11) (3.20.3)\n",
      "Collecting uritemplate<4,>=3.0.1 (from kfp==1.8.11)\n",
      "  Using cached uritemplate-3.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic<2,>=1.8.2 (from kfp==1.8.11)\n",
      "  Using cached pydantic-1.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
      "Collecting typer<1.0,>=0.3.2 (from kfp==1.8.11)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./.local/lib/python3.11/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.11) (1.16.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fire<1,>=0.3.1->kfp==1.8.11) (1.16.0)\n",
      "Collecting termcolor (from fire<1,>=0.3.1->kfp==1.8.11)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0 (from google-api-python-client<2,>=1.7.8->kfp==1.8.11)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2>=0.0.3 (from google-api-python-client<2,>=1.7.8->kfp==1.8.11)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.11) (2.12.0)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.1->kfp==1.8.11)\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.11) (0.3.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.11) (68.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.11) (4.9)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.31.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (2.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.11) (23.1.0)\n",
      "Collecting pyrsistent>=0.14.0 (from jsonschema<4,>=3.0.1->kfp==1.8.11)\n",
      "  Using cached pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in ./.local/lib/python3.11/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.11) (1.24.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.11) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.11) (2.8.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.11) (1.6.4)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.11) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<2,>=1.8.2->kfp==1.8.11) (4.8.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.11) (0.41.3)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.3.2->kfp==1.8.11)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.3.2->kfp==1.8.11)\n",
      "  Using cached rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.8.11) (1.61.0)\n",
      "INFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-core<3dev,>=1.21.0 (from google-api-python-client<2,>=1.7.8->kfp==1.8.11)\n",
      "  Using cached google_api_core-2.19.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.8.11)\n",
      "  Using cached proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core<3dev,>=1.21.0 (from google-api-python-client<2,>=1.7.8->kfp==1.8.11)\n",
      "  Using cached google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.17.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.17.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.16.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "INFO: pip is still looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_api_core-2.16.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.16.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached google_api_core-2.13.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached google_api_core-2.13.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.11.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.11.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached google_api_core-2.10.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.11/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.11) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.11) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.11) (3.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.3.2->kfp==1.8.11)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.3.2->kfp==1.8.11) (2.16.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.11) (3.2.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.3.2->kfp==1.8.11)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Using cached google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Using cached google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Using cached kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Using cached pydantic-1.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Using cached google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
      "Using cached rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: uritemplate, termcolor, strip-hints, shellingham, pyrsistent, pydantic, mdurl, kfp-pipeline-spec, httplib2, cloudpickle, cachetools, absl-py, markdown-it-py, kfp-server-api, jsonschema, google-auth, fire, rich, google-auth-httplib2, google-api-core, typer, google-api-python-client, google-cloud-storage, kfp\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.2.2\n",
      "    Uninstalling kfp-pipeline-spec-0.2.2:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.2.2\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.2\n",
      "    Uninstalling cachetools-5.3.2:\n",
      "      Successfully uninstalled cachetools-5.3.2\n",
      "  Attempting uninstall: kfp-server-api\n",
      "    Found existing installation: kfp-server-api 2.0.3\n",
      "    Uninstalling kfp-server-api-2.0.3:\n",
      "      Successfully uninstalled kfp-server-api-2.0.3\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.19.2\n",
      "    Uninstalling jsonschema-4.19.2:\n",
      "      Successfully uninstalled jsonschema-4.19.2\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.23.4\n",
      "    Uninstalling google-auth-2.23.4:\n",
      "      Successfully uninstalled google-auth-2.23.4\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.12.0\n",
      "    Uninstalling google-api-core-2.12.0:\n",
      "      Successfully uninstalled google-api-core-2.12.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.13.0\n",
      "    Uninstalling google-cloud-storage-2.13.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.13.0\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 2.4.0\n",
      "    Uninstalling kfp-2.4.0:\n",
      "      Successfully uninstalled kfp-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.25.0 requires jsonschema>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\n",
      "jupyter-events 0.8.0 requires jsonschema[format-nongpl]>=4.18.0, but you have jsonschema 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 cachetools-4.2.4 cloudpickle-2.2.1 fire-0.6.0 google-api-core-2.10.2 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.2.0 google-cloud-storage-1.44.0 httplib2-0.22.0 jsonschema-3.2.0 kfp-1.8.11 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 markdown-it-py-3.0.0 mdurl-0.1.2 pydantic-1.10.18 pyrsistent-0.20.0 rich-13.8.1 shellingham-1.5.4 strip-hints-0.1.10 termcolor-2.4.0 typer-0.12.5 uritemplate-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kfp==1.8.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9686d8be-e551-4d8e-8031-67c54bd5c28f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b039f9e6-c29e-45e3-8d35-d0c27c5428cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.notebook\n",
    "import kfp.components as comp\n",
    "from kfp import compiler\n",
    "from kfp.components import func_to_container_op, InputPath, OutputPath\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add2ca87-793c-4565-ad5d-8c81b747fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(out_dir: OutputPath(str),dataOrigin:str):\n",
    "    import pandas as pd\n",
    "    import os \n",
    "    \n",
    "    dataURL = \"https://raw.githubusercontent.com/Durbek-Gafur/noshowdata/main/\"+dataOrigin\n",
    "    NoShowData = pd.read_csv(dataURL)\n",
    "    #Convert the variable \"Gender\" to category\n",
    "    NoShowData['Gender'] = NoShowData['Gender'].astype('category')\n",
    "\n",
    "    #Convert the variable \"DOW\" to category\n",
    "    NoShowData['DOW'] = NoShowData['DOW'].astype('category')\n",
    "\n",
    "    #Convert the variable \"SMS_received\" to category\n",
    "    NoShowData['SMS_received'] = NoShowData['SMS_received'].astype('category')\n",
    "\n",
    "    #Convert the variable \"Scholarship\" to category\n",
    "    NoShowData['Scholarship'] = NoShowData['Scholarship'].astype('category')\n",
    "\n",
    "    #Convert the variable \"Smoking_Status\" to category\n",
    "    NoShowData['Smoking_Status'] = NoShowData['Smoking_Status'].astype('category')\n",
    "\n",
    "    #Convert the variable \"Hypertension\" to category\n",
    "    NoShowData['Hypertension'] = NoShowData['Hypertension'].astype('category')\n",
    "\n",
    "    #Convert the variable \"Diabetes\" to category\n",
    "    NoShowData['Diabetes'] = NoShowData['Diabetes'].astype('category')\n",
    "\n",
    "    #Convert the variable \"Alcoholism\" to category\n",
    "    NoShowData['Alcoholism'] = NoShowData['Alcoholism'].astype('category')\n",
    "\n",
    "    #Convert the variable \"Tuberculosis\" to category\n",
    "    NoShowData['Tuberculosis'] = NoShowData['Tuberculosis'].astype('category')\n",
    "    \n",
    "    #Dummy code the columns\n",
    "    try:\n",
    "        NoShowData = pd.get_dummies(NoShowData,\n",
    "        columns=[\"Gender\",\"DOW\",\"SMS_received\", \"Scholarship\", \"Smoking_Status\", \"Hypertension\", \"Diabetes\", \"Alcoholism\", \"Tuberculosis\", \"Status\"],\n",
    "        prefix=[\"Gender\",\"DOW\",\"SMS_received\", \"Scholarship\", \"Smoking_Status\", \"Hypertension\", \"Diabetes\", \"Alcoholism\", \"Tuberculosis\", \"Status\"], \n",
    "                                 drop_first = True)\n",
    "    except:\n",
    "        NoShowData = pd.get_dummies(NoShowData,\n",
    "        columns=[\"Gender\",\"DOW\",\"SMS_received\", \"Scholarship\", \"Smoking_Status\", \"Hypertension\", \"Diabetes\", \"Alcoholism\", \"Tuberculosis\"],\n",
    "        prefix=[\"Gender\",\"DOW\",\"SMS_received\", \"Scholarship\", \"Smoking_Status\", \"Hypertension\", \"Diabetes\", \"Alcoholism\", \"Tuberculosis\"], \n",
    "                                 drop_first = True)\n",
    "    NoShowData.to_csv(out_dir, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e7f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectFeatureAndSplit(in_dir: InputPath(),\n",
    "                          x_train: OutputPath(str), \n",
    "                          x_test: OutputPath(str),\n",
    "                          y_train: OutputPath(str), \n",
    "                          y_test: OutputPath(str)):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Read the input dataset\n",
    "    NoShowData = pd.read_csv(in_dir)\n",
    "    \n",
    "    # Split the data into predictors and outcome\n",
    "    NoShow_Predictors = pd.DataFrame(NoShowData.iloc[:,:-1])\n",
    "    NoShow_Outcome = pd.DataFrame(NoShowData.iloc[:,-1])\n",
    "    \n",
    "    # Perform train/test split\n",
    "    X_Train_NoShow, X_Test_NoShow, y_Train_NoShow, y_Test_NoShow = train_test_split(\n",
    "        NoShow_Predictors, \n",
    "        NoShow_Outcome, \n",
    "        test_size=0.25, \n",
    "        random_state=8810\n",
    "    )\n",
    "\n",
    "    # Ensure the directories exist for saving outputs\n",
    "    os.makedirs(os.path.dirname(x_train), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(x_test), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(y_train), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(y_test), exist_ok=True)\n",
    "    \n",
    "    # Save the output datasets\n",
    "    X_Train_NoShow.to_csv(x_train, index=False)\n",
    "    X_Test_NoShow.to_csv(x_test, index=False)\n",
    "    y_Train_NoShow.to_csv(y_train, index=False)\n",
    "    y_Test_NoShow.to_csv(y_test, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb8abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainClassifier(x_train_dir: InputPath(),y_train_dir: InputPath(), out_dir: OutputPath(str),classifierName:str):\n",
    "    import pandas as pd\n",
    "    import os \n",
    "    if classifierName == \"DecisionTreeClassifier\":\n",
    "        from sklearn.tree import DecisionTreeClassifier as Classifier\n",
    "    elif classifierName == \"RandomForestClassifier\":\n",
    "        from sklearn.ensemble import RandomForestClassifier as Classifier\n",
    "    import pickle\n",
    "    \n",
    "    X_Train_NoShow = pd.read_csv(x_train_dir)\n",
    "    y_Train_NoShow = pd.read_csv(y_train_dir)\n",
    "    model = Classifier()\n",
    "    if classifierName == \"DecisionTreeClassifier\":\n",
    "        model = model.fit(X_Train_NoShow, y_Train_NoShow)\n",
    "    elif classifierName == \"RandomForestClassifier\":\n",
    "        model = model.fit(X_Train_NoShow, y_Train_NoShow.values.ravel())\n",
    "    \n",
    "    with open(out_dir, 'wb') as handle:\n",
    "        pickle.dump(model, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d843b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestClassifier(pickle_dir: InputPath(),x_test_dir: InputPath(),y_test_dir: InputPath(),classifierName: str) -> float:\n",
    "    import pandas as pd\n",
    "    import os \n",
    "    if classifierName == \"DecisionTreeClassifier\":\n",
    "        from sklearn.tree import DecisionTreeClassifier as Classifier\n",
    "    elif classifierName == \"RandomForestClassifier\":\n",
    "        from sklearn.ensemble import RandomForestClassifier as Classifier\n",
    "    from sklearn import metrics\n",
    "    import pickle\n",
    "    \n",
    "    with open(pickle_dir, 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    X_Test_NoShow = pd.read_csv(x_test_dir)\n",
    "    y_Test_NoShow = pd.read_csv(y_test_dir)\n",
    "    \n",
    "    y_pred = model.predict(X_Test_NoShow)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_Test_NoShow, y_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return float(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11575579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SmartScheduling(pickle_dir: InputPath(), data_dir: InputPath(),classifierName:str) -> str:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    import pickle\n",
    "    \n",
    "    with open(pickle_dir, 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "        \n",
    "    NoShowData_NewExamples = pd.read_csv(data_dir)\n",
    "    #simulate 20 patient calls and predict their no-show risk\n",
    "    tot_patients = 20\n",
    "    deferred = 0 #initialize deferred to 0\n",
    "    patient_info = NoShowData_NewExamples.sample(tot_patients) #randomly sample 20 patient from the excel file for scheduling\n",
    "    risk_predictions = model.predict(patient_info)\n",
    "\n",
    "    #model parameters:\n",
    "    tot_slots = 10 #total number of available slots for booking\n",
    "    DB = 0 #initilize total double-booked slots to 0\n",
    "    DB_max = 3 #only 3 slots can be double booked (30% of 10 slots)\n",
    "    deferred = 0 #initialize deferred appointments to zero\n",
    "\n",
    "    #initialize the appointment schedule\n",
    "    appointment_schedule = np.empty((tot_slots))\n",
    "    appointment_schedule[:] = np.NaN\n",
    "\n",
    "    appointment_schedule_DB = np.empty((tot_slots))\n",
    "    appointment_schedule_DB[:] = np.NaN\n",
    "\n",
    "    slot_capacity = np.zeros((tot_slots))\n",
    "    slot_capacity.fill(2) #no more than 2 patients per slot\n",
    "\n",
    "    slot_risktype = np.zeros((tot_slots)) #risk type of patient scheduled in a slot\n",
    "    slot_risktype.fill(2) \n",
    "\n",
    "    for p in range(tot_patients): #simulates sequential patient call-in. (i.e., for each patient calling for an appointment)\n",
    "        assignment = 0\n",
    "        if risk_predictions[p] == 1: #patient is low-risk\n",
    "            for slot in range(tot_slots): #start from beginning to search for a slot\n",
    "                if slot_capacity[slot] == 2 and assignment==0:\n",
    "                    appointment_schedule[slot] = p\n",
    "                    assignment = 1\n",
    "                    slot_risktype[slot] = 1 #risk type of patient single booked in this slot is low-risk\n",
    "                    slot_capacity[slot] = slot_capacity[slot] - 1\n",
    "\n",
    "            if assignment == 0 and DB < DB_max: #scan for double-booking \n",
    "                for slot in range(tot_slots): #start from beginning to search for first feasible slot according to overbooking policy\n",
    "                    if slot_capacity[slot] == 1 and slot_risktype[slot] == 0 and assignment==0:\n",
    "                        appointment_schedule_DB[slot] = p\n",
    "                        assignment = 1\n",
    "                        slot_capacity[slot] = slot_capacity[slot] - 1\n",
    "                        DB = DB + 1\n",
    "\n",
    "            if assignment == 0: #if patient is still not scheduled then assign it to \n",
    "                deferred = deferred+1\n",
    "\n",
    "\n",
    "        if risk_predictions[p] == 0: #patient is high-risk\n",
    "            for slot in range(tot_slots-1,-1,-1): #start from end to search for a slot\n",
    "                if slot_capacity[slot] == 2 and assignment==0:\n",
    "                    appointment_schedule[slot] = p\n",
    "                    assignment = 1\n",
    "                    slot_risktype[slot] = 0 #risk type of patient single booked in this slot is high-risk\n",
    "                    slot_capacity[slot] = slot_capacity[slot] - 1\n",
    "\n",
    "            if assignment == 0 and DB < DB_max: #scan for double-booking \n",
    "                for slot in range(tot_slots-1,-1,-1): #start from beginning to search for first feasible slot according to overbooking policy\n",
    "                    if slot_capacity[slot] == 1 and slot_risktype[slot] == 1 and assignment==0:\n",
    "                        appointment_schedule_DB[slot] = p\n",
    "                        assignment = 1\n",
    "                        slot_capacity[slot] = slot_capacity[slot] - 1\n",
    "                        DB = DB + 1\n",
    "\n",
    "            if assignment == 0: #if patient is still not scheduled then assign it to \n",
    "                deferred = deferred+1\n",
    "    with open('output.txt', 'w') as f:\n",
    "        print(f\"Schedule Generated: {appointment_schedule}\\nDoubleBooked Slots: {appointment_schedule_DB}\\nDeferred Patients: {deferred}\",file=f)\n",
    "        \n",
    "    return f\"Schedule Generated: {appointment_schedule}\\nDoubleBooked Slots: {appointment_schedule_DB}\\nDeferred Patients: {deferred}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10d6a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetImportantFeatures(pickle_dir: InputPath(), data_dir: InputPath(), classifierName: str ) -> str:\n",
    "    import pandas as pd\n",
    "    import os \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt \n",
    "    import pickle\n",
    "    import sklearn\n",
    "    \n",
    "    with open(pickle_dir, 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    \n",
    "    NoShowData = pd.read_csv(data_dir)\n",
    "    features = list(NoShowData.columns[:-1])\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.title('Feature Importances')\n",
    "    r = \"\"\n",
    "    for i in indices:\n",
    "        r += f\"{features[i]}\\t:\\t{importances[i]}\\n\"\n",
    "            \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976a7ece-c425-408e-8038-583e526fa082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvalClassifiers(r1:float, r2:float, pickle1: InputPath(),pickle2: InputPath(), out_dir: OutputPath(str)):\n",
    "    import pickle\n",
    "    import sklearn\n",
    "    if r1>r2:\n",
    "        p = pickle1\n",
    "    else:\n",
    "        p = pickle2\n",
    "    with open(p, 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "    with open(out_dir, 'wb') as handle:\n",
    "        pickle.dump(model, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b080175",
   "metadata": {},
   "source": [
    "## Turning function to container_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1afed6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_op = comp.func_to_container_op(Preprocess,\n",
    "                                              base_image='tensorflow/tensorflow:latest',\n",
    "                                              packages_to_install=['pandas'])  \n",
    "\n",
    "selectFeatureAndSplit_op = comp.func_to_container_op(SelectFeatureAndSplit,\n",
    "                                              base_image='tensorflow/tensorflow:latest',\n",
    "                                              packages_to_install=['pandas','scikit-learn'])  \n",
    "\n",
    "trainClassifier_op = comp.func_to_container_op(TrainClassifier,\n",
    "                                              base_image='tensorflow/tensorflow:latest',\n",
    "                                              packages_to_install=['pandas','scikit-learn'])  \n",
    "\n",
    "testClassifier_op = comp.func_to_container_op(TestClassifier,\n",
    "                                              base_image='tensorflow/tensorflow:latest',\n",
    "                                              packages_to_install=['pandas','scikit-learn','pickle-mixin'])  \n",
    "\n",
    "smartScheduling_op = comp.func_to_container_op(SmartScheduling,\n",
    "                                              base_image='tensorflow/tensorflow:latest',\n",
    "                                              packages_to_install=['pandas','scikit-learn','numpy','pickle-mixin']) \n",
    "\n",
    "getImportantFeatures_op = comp.func_to_container_op(GetImportantFeatures,\n",
    "                                              base_image='tensorflow/tensorflow:latest',\n",
    "                                              packages_to_install=['pandas','matplotlib','numpy','pickle-mixin','scikit-learn'])  \n",
    "\n",
    "evalClassifiers_op = comp.func_to_container_op(EvalClassifiers,\n",
    "                                              base_image='tensorflow/tensorflow:latest',\n",
    "                                              packages_to_install=['pickle-mixin','scikit-learn'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2fcad",
   "metadata": {},
   "source": [
    "## Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10cbdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Smart Scheduling \",\n",
    "    description=\"Smart Outpatient Appointment Scheduling System\"\n",
    ")\n",
    "def smart_scheduling():\n",
    "    \n",
    "    # Pipeline's task 1 : Download and preprocess data\n",
    "    preprocess_task = preprocess_op(\"No-show_Data.csv\")\n",
    " \n",
    "    # Pipeline's task 2 : Feature Selection and Split Data into Training and Testing\n",
    "    selectFeatureAndSplit_task = selectFeatureAndSplit_op(preprocess_task.output)\n",
    "\n",
    "    # Pipeline's task 3 : Decision Tree Classififer Training\n",
    "    trainClassifier_op_DT_task = trainClassifier_op(selectFeatureAndSplit_task.outputs[\"x_train\"],selectFeatureAndSplit_task.outputs[\"y_train\"],\"DecisionTreeClassifier\")\n",
    " \n",
    "    # Pipeline's task 3 : Random Forest Classifier Training\n",
    "    trainClassifier_op_RF_task = trainClassifier_op(selectFeatureAndSplit_task.outputs[\"x_train\"],selectFeatureAndSplit_task.outputs[\"y_train\"],\"RandomForestClassifier\")\n",
    "\n",
    "    # Pipeline's task 4 : Test Decision Tree Classififer \n",
    "    testClassifier_op_DT_task = testClassifier_op(trainClassifier_op_DT_task.output,selectFeatureAndSplit_task.outputs[\"x_test\"],selectFeatureAndSplit_task.outputs[\"y_test\"],\"DecisionTreeClassifier\")\n",
    " \n",
    "    # Pipeline's task 4 : Test Random Forest Classifier \n",
    "    testClassifier_op_RF_task = testClassifier_op(trainClassifier_op_RF_task.output,selectFeatureAndSplit_task.outputs[\"x_test\"],selectFeatureAndSplit_task.outputs[\"y_test\"],\"RandomForestClassifier\")\n",
    "    \n",
    "    evalClassifiers_task = evalClassifiers_op(testClassifier_op_DT_task.output,testClassifier_op_RF_task.output,trainClassifier_op_DT_task.output,trainClassifier_op_RF_task.output)\n",
    "    # Select Best Classifier \n",
    "    if evalClassifiers_task.output == trainClassifier_op_DT_task.output:\n",
    "        best_classifier = \"DecisionTreeClassifier\"\n",
    "    else:\n",
    "        best_classifier = \"RandomForestClassifier\"\n",
    "        \n",
    "    # Pipeline's task 5 : Identify Variables Important for Predicting No-shows\n",
    "    getImportantFeatures_task = getImportantFeatures_op(evalClassifiers_task.output,preprocess_task.output, best_classifier)\n",
    "    \n",
    "    # Pipeline's task 6 : Predict New Examples\n",
    "    preprocess_new_task = preprocess_op(\"No-show_Data_Testing.csv\")\n",
    "\n",
    "    # Pipeline's task 7 : Smart Scheduling according to prediction\n",
    "    smartScheduling_op(evalClassifiers_task.output, preprocess_new_task.output, best_classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22a758",
   "metadata": {},
   "source": [
    "## Execute pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc8cdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(smart_scheduling, \"smart_scheduling.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23515c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 8469 Sep 13 22:43 ./smart_scheduling.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./smart_scheduling.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8732e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./smart_scheduling.zip\n",
      "  inflating: pipeline.yaml           \n"
     ]
    }
   ],
   "source": [
    "!unzip -o ./smart_scheduling.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2105e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0649d396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/a7d303ed-76eb-44ef-966f-f2a6f36b4ee5\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "EXPERIMENT_NAME = \"Smart Scheduling Experiment 3\"\n",
    "client = kfp.Client()\n",
    "try:\n",
    "    experiment = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "my_run = client.run_pipeline(experiment.id, \"smart-scheduling-pipeline\", \"smart_scheduling.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cef71-beac-44a8-868d-9660d0509005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63121b14-7284-45b0-92df-16ec5d6f4f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "deploy_config": {},
   "docker_image": "gcr.io/arrikto/jupyter-kale-py38@sha256:b7e923046b834491fb5fd90850940cb3b57ba5aedb7d558757d9848db6cb28eb",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": true,
   "storage_class_name": "",
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "dgvkh-jupyter-workspace-zln2x",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
